import json
import os
import re
import uuid
import shutil
import tempfile
import subprocess
from urllib.parse import quote_plus, urljoin, urlparse, parse_qs
import requests
from typing import Dict, Any, List, Optional, Union
from urllib.parse import unquote

from bs4 import BeautifulSoup
import json as _json_unescape  # for decoding escaped strings in JSON blobs
import xml.etree.ElementTree as ET
import torchaudio
# pyannote 3.3.x AudioDecoder ì´ë¦„ ì˜¤ë¥˜ ë°©ì–´ìš© + torchcodec ë¯¸ì„¤ì¹˜ ëŒ€ë¹„
try:
    from pyannote.audio.core.io import AudioDecoder  # type: ignore
except Exception:
    AudioDecoder = None  # type: ignore
try:
    import pyannote.audio.core.io as _pyannote_io  # type: ignore
except Exception:
    _pyannote_io = None
if _pyannote_io is not None and getattr(_pyannote_io, "AudioDecoder", None) is None:
    import torch

    class _FallbackAudioStreamMetadata:
        def __init__(self, sample_rate: int, num_frames: int, num_channels: int):
            self.sample_rate = sample_rate
            self.num_frames = num_frames
            self.num_channels = num_channels
            self.duration_seconds_from_header = num_frames / sample_rate if sample_rate else 0.0

    class _FallbackAudioSamples:
        def __init__(self, data: torch.Tensor, sample_rate: int):
            self.data = data
            self.sample_rate = sample_rate

    class _FallbackAudioDecoder:
        def __init__(self, source):
            self.source = source
            import soundfile as sf
            import numpy as np

            data, sr = sf.read(source, always_2d=True)
            # soundfile returns [frames, channels]; convert to [channel, time]
            data = np.asarray(data, dtype="float32").T
            waveform = torch.from_numpy(data)
            self._waveform = waveform
            self._sr = sr
            self.metadata = _FallbackAudioStreamMetadata(sr, waveform.shape[1], waveform.shape[0])

        def get_all_samples(self):
            return _FallbackAudioSamples(self._waveform, self._sr)

        def get_samples_played_in_range(self, start: float, end: float):
            start_frame = int(max(0, start * self._sr))
            end_frame = int(min(self._waveform.shape[1], end * self._sr))
            data = self._waveform[:, start_frame:end_frame]
            return _FallbackAudioSamples(data, self._sr)

    _pyannote_io.AudioDecoder = _FallbackAudioDecoder  # type: ignore
    _pyannote_io.AudioStreamMetadata = _FallbackAudioStreamMetadata  # type: ignore
    _pyannote_io.AudioSamples = _FallbackAudioSamples  # type: ignore
    AudioDecoder = _FallbackAudioDecoder  # type: ignore
from dotenv import load_dotenv
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from pyannote.audio import Pipeline
from pyannote.audio.core.model import Model
from faster_whisper import WhisperModel
from summa.summarizer import summarize

# =========================
# í™˜ê²½ ë³€ìˆ˜ / ì „ì—­ ì„¤ì •
# =========================

load_dotenv()

# ì¼ë¶€ ë°°í¬ í™˜ê²½ì—ì„œëŠ” torchaudioì— list_audio_backendsê°€ ì—†ì–´ speechbrain ì´ˆê¸°í™”ê°€ ì‹¤íŒ¨í•˜ë¯€ë¡œ ë°©ì–´ì ìœ¼ë¡œ ì¶”ê°€
if not hasattr(torchaudio, "list_audio_backends"):
    torchaudio.list_audio_backends = lambda: []

# pyannote ë‚´ë¶€ì—ì„œ Model.from_pretrainedì— 'repo@rev' ë¬¸ìì—´ì„ ë°”ë¡œ ë„˜ê¸°ëŠ” ê²½ìš°ê°€ ìˆì–´
# revision ì¸ìë¥¼ ìë™ ë¶„ë¦¬í•˜ë„ë¡ monkey patch
_orig_model_from_pretrained = Model.from_pretrained


def _patched_model_from_pretrained(checkpoint, *args, revision=None, **kwargs):
    if isinstance(checkpoint, str) and "@" in checkpoint and revision is None:
        base, rev = checkpoint.split("@", 1)
        return _orig_model_from_pretrained(base, *args, revision=rev, **kwargs)
    return _orig_model_from_pretrained(checkpoint, *args, revision=revision, **kwargs)


Model.from_pretrained = staticmethod(_patched_model_from_pretrained)  # type: ignore[assignment]

HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
if HF_TOKEN is None:
    raise RuntimeError("HUGGINGFACE_TOKEN í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

def _parse_model_id_and_revision(model_id: str) -> tuple[str, Optional[str]]:
    """í™˜ê²½ ë³€ìˆ˜ì— 'repo@rev' í˜•íƒœê°€ ë“¤ì–´ì˜¤ë©´ revisionìœ¼ë¡œ ë¶„ë¦¬."""
    if "@" in model_id:
        base, revision = model_id.split("@", 1)
        return base, revision
    return model_id, None

PYANNOTE_MODEL_ID_RAW = os.getenv(
    "PYANNOTE_MODEL_ID",
    "pyannote/speaker-diarization"  # í•„ìš”í•˜ë©´ ë‹¤ë¥¸ ëª¨ë¸ IDë¡œ ë°”ê¿”ë„ ë¨
)
PYANNOTE_MODEL_ID, PYANNOTE_MODEL_REVISION = _parse_model_id_and_revision(PYANNOTE_MODEL_ID_RAW)

WHISPER_MODEL_SIZE = os.getenv("WHISPER_MODEL_SIZE", "medium")  # tiny/small/medium/large ë“±
USE_GPU = os.getenv("USE_GPU", "false").lower() == "true"
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
# ê¸°ë³¸ê°’ì„ v1 APIìš© ìµœì‹  í”Œë˜ì‹œë¡œ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ë¡œ ë®ì–´ì“°ê¸° ê°€ëŠ¥)
GEMINI_MODEL_ID = os.getenv("GEMINI_MODEL_ID", "gemini-2.5-flash")
# ê¸°ë³¸ API ë²„ì „ (ê¶Œì¥ v1, í•„ìš” ì‹œ v1beta)
GEMINI_API_VERSION = os.getenv("GEMINI_API_VERSION", "v1")

# =========================
# ëª¨ë¸ ë¡œë”© (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ)
# =========================

print("Loading pyannote pipeline...")
diarization_pipeline = Pipeline.from_pretrained(
    PYANNOTE_MODEL_ID,
    token=HF_TOKEN,
    revision=PYANNOTE_MODEL_REVISION
)

print("Loading Whisper model...")
whisper_model = WhisperModel(
    WHISPER_MODEL_SIZE,
    device="cuda" if USE_GPU else "cpu",
    compute_type="int8"  # ì†ë„/ë©”ëª¨ë¦¬ ì ˆì•½ìš©
)

app = FastAPI(title="Meeting Summarizer API", version="0.1.0")

# CORS ì„¤ì •
origins = [
    "http://localhost:3000",
    "https://v2.muldum.com",
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =========================
# Gemini ì‡¼í•‘ ì¶”ì²œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
# =========================

GEMINI_SYSTEM_PROMPT = (
    "ë„ˆëŠ” DeviceMart/11ë²ˆê°€ ê°€ê²© ë¹„êµë¥¼ ë•ëŠ” ì‡¼í•‘ ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. "
    "ë¬´ë£Œë°°ì†¡ ìƒí’ˆì„ ìµœìš°ì„ ìœ¼ë¡œ, ê°™ì€ ì¡°ê±´ì´ë¼ë©´ ë” ì €ë ´í•œ ìƒí’ˆì„ ì¶”ì²œí•œë‹¤. "
    "ì§€ì •ëœ JSON ì™¸ì˜ ë§ì€ ì ˆëŒ€ í•˜ì§€ ì•ŠëŠ”ë‹¤."
)

GEMINI_RESPONSE_SCHEMA_EXAMPLE = {
    "summary": "ê±°ì ˆ ì‚¬ìœ ë¥¼ ì–´ë–»ê²Œ ê°œì„ í–ˆëŠ”ì§€ 1-2ë¬¸ì¥",
    "recommendations": [
        {
            "itemId": 123,
            "productName": "ìƒí’ˆëª…",
            "source": "DeviceMart | 11ë²ˆê°€",
            "price": 19000,
            "deliveryPrice": "ë¬´ë£Œë°°ì†¡ or 2000",
            "estimatedDelivery": "2025-02-03",
            "productUrl": "https://...",
            "imageUrl": "https://...",
            "reason": "ê°€ê²©/ë°°ì†¡/ê±°ì ˆì‚¬ìœ  ê°œì„  í¬ì¸íŠ¸ í•œ ì¤„"
        }
    ]
}


class BaseItem(BaseModel):
    id: int
    product_name: str = Field(..., alias="productName")
    price: int
    link: str
    # teamIdê°€ ìˆ«ì/ë¬¸ì ëª¨ë‘ ë“¤ì–´ì˜¤ëŠ” ìƒí™©ì„ í—ˆìš©
    team_id: Optional[Union[str, int]] = Field(None, alias="teamId")
    reject_reason: Optional[str] = Field(None, alias="rejectReason")

    class Config:
        allow_population_by_field_name = True


class CandidateItem(BaseModel):
    item_id: int = Field(..., alias="itemId")
    product_name: str = Field(..., alias="productName")
    price: int
    delivery_price: Optional[str] = Field(None, alias="deliveryPrice")
    delivery_time: Optional[str] = Field(None, alias="deliveryTime")
    link: Optional[str] = None
    image_url: Optional[str] = Field(None, alias="imageUrl")
    recent_registered_at: Optional[str] = Field(None, alias="recentRegisteredAt")
    same_team: Optional[bool] = Field(None, alias="sameTeam")
    source: Optional[str] = Field(None, description="DeviceMart ë˜ëŠ” 11ë²ˆê°€")

    class Config:
        allow_population_by_field_name = True


class RecommendationRequest(BaseModel):
    base_item: BaseItem = Field(..., alias="baseItem")
    candidates: List[CandidateItem] = Field(default_factory=list)

    class Config:
        allow_population_by_field_name = True


def _model_dump(model: BaseModel) -> Dict[str, Any]:
    """pydantic v1/v2 í˜¸í™˜ìš© dict ì¶”ì¶œ"""
    if hasattr(model, "model_dump"):
        return model.model_dump(by_alias=True, exclude_none=True)  # type: ignore[attr-defined]
    return model.dict(by_alias=True, exclude_none=True)


def build_recommendation_prompt(
    base_item: BaseItem,
    candidates: List[CandidateItem]
) -> Dict[str, Any]:
    """
    Geminiì— ì „ë‹¬í•  system/user í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œë¥¼ êµ¬ì„±í•œë‹¤.
    """
    # ê¸°ì¤€ ìƒí’ˆê³¼ ë™ì¼í•œ ID/ì´ë¦„ì€ í›„ë³´ì—ì„œ ì œì™¸
    candidate_payload = []
    for c in candidates:
        if c.item_id == base_item.id:
            continue
        if c.product_name.strip() == base_item.product_name.strip():
            continue
        candidate_payload.append(_model_dump(c))

    reject_reason = base_item.reject_reason or "ì—†ìŒ"
    base_link = base_item.link or "ì—†ìŒ"
    has_candidates = len(candidate_payload) > 0

    if has_candidates:
        user_prompt = (
            "ê¸°ì¤€ ìƒí’ˆ:\n"
            f"- ì´ë¦„: {base_item.product_name}\n"
            f"- ê°€ê²©: {base_item.price}\n"
            f"- ë§í¬: {base_link}\n"
            f"- íŒ€ID: {base_item.team_id or 'ë¯¸ì§€ì •'}\n"
            f"- ê±°ì ˆ ì‚¬ìœ : {reject_reason}\n\n"
            "í›„ë³´ ëª©ë¡(JSON ë°°ì—´):\n"
            f"{json.dumps(candidate_payload, ensure_ascii=False, indent=2)}\n\n"
            "ìš”ì²­:\n"
            "- ê¸°ì¤€ ìƒí’ˆê³¼ ë™ì¼í•œ ì´ë¦„/IDì˜ í›„ë³´ëŠ” ì¶”ì²œì—ì„œ ì œì™¸.\n"
            "- ê° ì¶”ì²œì€ productUrl(í›„ë³´ì˜ link)ê³¼ imageUrl(í›„ë³´ì˜ imageUrl)ì„ ë°˜ë“œì‹œ í¬í•¨. ì—†ìœ¼ë©´ í•´ë‹¹ í›„ë³´ëŠ” ì œì™¸í•˜ê±°ë‚˜ í•©ë¦¬ì ì¸ ê°’ìœ¼ë¡œ ì±„ì›Œ.\n"
            "1) ë¬´ë£Œë°°ì†¡ì´ ì•„ë‹Œ í›„ë³´ëŠ” ì œì™¸í•˜ê³  ìµœëŒ€ 3ê°œë§Œ ì¶”ì²œ. ë‹¨, ëª¨ë‘ ìœ ë£Œë°°ì†¡ì´ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œí•˜ê³  ìµœì €ê°€ ìˆœìœ¼ë¡œ 3ê°œ ì¶”ì²œ.\n"
            "2) ê° ì¶”ì²œë§ˆë‹¤ (ê°€ê²©, ë°°ì†¡ë¹„, ì¶”ì²œ ì´ìœ )ì„ í•œ ì¤„ ìš”ì•½ìœ¼ë¡œ ì„¤ëª….\n"
            f"3) ê±°ì ˆ ì‚¬ìœ ({reject_reason})ë¥¼ í•´ê²°/ê°œì„ í•˜ëŠ” í›„ë³´ë§Œ ê³ ë¥¸ë‹¤.\n"
            "4) JSONìœ¼ë¡œë§Œ ë‹µí•˜ê³ , ë‹¤ìŒ ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥¸ë‹¤."
        )
    else:
        user_prompt = (
            "ê¸°ì¤€ ìƒí’ˆ:\n"
            f"- ì´ë¦„: {base_item.product_name}\n"
            f"- ê°€ê²©: {base_item.price}\n"
            f"- ë§í¬: {base_link}\n"
            f"- íŒ€ID: {base_item.team_id or 'ë¯¸ì§€ì •'}\n"
            f"- ê±°ì ˆ ì‚¬ìœ : {reject_reason}\n\n"
            "í›„ë³´ ëª©ë¡ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n"
            "ìš”ì²­:\n"
            "- ê¸°ì¤€ ìƒí’ˆê³¼ ë™ì¼í•œ ì´ë¦„/IDëŠ” ì ˆëŒ€ ì¶”ì²œí•˜ì§€ ë§ ê²ƒ.\n"
            "- productUrl, imageUrlì„ í•©ë¦¬ì ì¸ ê°’(ì˜ˆ: https://example.com/item/..., https://example.com/img/...)ìœ¼ë¡œ ì±„ì›Œì„œ ìµœëŒ€ 3ê°œ ì œì•ˆ.\n"
            "1) ê¸°ì¤€ ìƒí’ˆê³¼ ê±°ì ˆ ì‚¬ìœ ë¥¼ ì°¸ê³ í•˜ì—¬ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ëŒ€ì²´ ìƒí’ˆì„ ìµœëŒ€ 3ê°œ ìƒì„±í•´ì„œ ì¶”ì²œ.\n"
            "2) ë¬´ë£Œë°°ì†¡ì„ ìš°ì„ , ì—†ìœ¼ë©´ ìœ ë£Œë°°ì†¡ì´ë¼ë„ ìµœì €ê°€ ìˆœìœ¼ë¡œ ì œì•ˆ.\n"
            "3) ê°€ê²©/ë°°ì†¡ë¹„/ë„ì°©ì˜ˆìƒì¼/ë§í¬/ì´ë¯¸ì§€ë¥¼ í•©ë¦¬ì  ê°’ìœ¼ë¡œ ì±„ì›Œ ë„£ë˜, ê±°ì ˆ ì‚¬ìœ ë¥¼ í•´ê²°í•˜ë„ë¡ ì„ íƒ.\n"
            "4) JSONìœ¼ë¡œë§Œ ë‹µí•˜ê³ , ë‹¤ìŒ ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥¸ë‹¤."
        )

    return {
        "system_prompt": GEMINI_SYSTEM_PROMPT,
        "user_prompt": user_prompt,
        "response_schema_example": GEMINI_RESPONSE_SCHEMA_EXAMPLE,
    }


def call_gemini(system_prompt: str, user_prompt: str) -> Dict[str, Any]:
    """
    Gemini REST API í˜¸ì¶œ. ì„±ê³µ ì‹œ ì›ë¬¸ê³¼ íŒŒì‹±ëœ JSON(ê°€ëŠ¥í•˜ë©´)ì„ ë°˜í™˜í•œë‹¤.
    """
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=500, detail="GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

    def _do_call(version: str):
        url = f"https://generativelanguage.googleapis.com/{version}/models/{GEMINI_MODEL_ID}:generateContent?key={GEMINI_API_KEY}"
        payload = {
            "contents": [
                {
                    "role": "user",
                    "parts": [
                        {"text": f"{system_prompt}\n\n{user_prompt}"}
                    ],
                }
            ],
            "generationConfig": {
                "temperature": 0.3,
            },
        }
        try:
            return requests.post(url, json=payload, timeout=60)
        except Exception as e:
            raise HTTPException(status_code=502, detail=f"Gemini í˜¸ì¶œ ì‹¤íŒ¨: {e}")

    # ìš°ì„  ì„¤ì •ëœ ë²„ì „ìœ¼ë¡œ í˜¸ì¶œ, 404ë©´ v1<->v1beta êµì°¨ ì¬ì‹œë„
    resp = _do_call(GEMINI_API_VERSION)
    if resp.status_code == 404 and GEMINI_API_VERSION == "v1beta":
        resp = _do_call("v1")
    elif resp.status_code == 404 and GEMINI_API_VERSION == "v1":
        resp = _do_call("v1beta")

    if resp.status_code != 200:
        if resp.status_code == 429:
            raise HTTPException(status_code=429, detail="Gemini í˜¸ì¶œì´ ê³¼ë„í•©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        raise HTTPException(status_code=resp.status_code, detail=f"Gemini ì˜¤ë¥˜: {resp.text}")

    data = resp.json()
    try:
        text = data["candidates"][0]["content"]["parts"][0]["text"]
    except Exception:
        raise HTTPException(status_code=502, detail="Gemini ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨")

    parsed_json: Optional[Dict[str, Any]] = None
    cleaned = text.strip()
    if cleaned.startswith("```"):
        # ```json ... ``` í˜•íƒœ ì œê±°
        cleaned_lines = cleaned.splitlines()
        if cleaned_lines and cleaned_lines[0].startswith("```"):
            cleaned_lines = cleaned_lines[1:]
        if cleaned_lines and cleaned_lines[-1].startswith("```"):
            cleaned_lines = cleaned_lines[:-1]
        cleaned = "\n".join(cleaned_lines).strip()
    try:
        parsed_json = json.loads(cleaned)
    except Exception:
        parsed_json = None

    return {
        "raw": text,
        "parsed": parsed_json,
    }


# =========================
# ìœ í‹¸ í•¨ìˆ˜ë“¤
# =========================

def save_upload_file_tmp(upload_file: UploadFile) -> str:
    """ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ ê²½ë¡œì— ì €ì¥í•˜ê³  íŒŒì¼ ê²½ë¡œ ë¦¬í„´"""
    suffix = os.path.splitext(upload_file.filename or "")[1]
    tmp_fd, tmp_path = tempfile.mkstemp(suffix=suffix)
    with os.fdopen(tmp_fd, "wb") as tmp:
        shutil.copyfileobj(upload_file.file, tmp)
    return tmp_path


def cut_audio_segment(
    input_path: str,
    start: float,
    end: float,
    output_path: str,
    sample_rate: int = 16000
) -> None:
    """
    ffmpegë¥¼ ì´ìš©í•´ íŠ¹ì • êµ¬ê°„ë§Œ ì˜ë¼ë‚´ê¸°
    - start, end: ì´ˆ ë‹¨ìœ„
    - mono, 16kHzë¡œ ë¦¬ìƒ˜í”Œë§
    """
    cmd = [
        "ffmpeg",
        "-y",  # ë®ì–´ì“°ê¸°
        "-i", input_path,
        "-ss", str(start),
        "-to", str(end),
        "-ar", str(sample_rate),
        "-ac", "1",
        "-f", "wav",
        output_path
    ]
    result = subprocess.run(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    if result.returncode != 0:
        raise RuntimeError(f"ffmpeg error: {result.stderr.decode('utf-8')}")


def summarize_text(text: str, ratio: float = 0.2, max_sentences: int = 5) -> str:
    """
    TextRank ê¸°ë°˜ ì¶”ì¶œ ìš”ì•½.
    - í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê·¸ëƒ¥ ì›ë¬¸ ë¦¬í„´.
    """
    text = (text or "").strip()
    if not text:
        return ""

    # ëŒ€ì¶© ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ìš”ì•½ ì‹œë„ ì—¬ë¶€ íŒë‹¨ (í•„ìš”í•˜ë©´ ë” ê³ ê¸‰ ë¡œì§ìœ¼ë¡œ ë°”ê¿”ë„ ë¨)
    if len(text.split()) < 30:
        return text

    try:
        summary = summarize(text, ratio=ratio)
        if not summary:
            return text

        # ë¬¸ì¥ ìˆ˜ ì œí•œ
        sentences = [s.strip() for s in summary.split("\n") if s.strip()]
        if len(sentences) > max_sentences:
            sentences = sentences[:max_sentences]
        return " ".join(sentences)
    except Exception:
        # summa ë‚´ë¶€ ì—ëŸ¬ ì‹œ ê·¸ëƒ¥ ì›ë¬¸ ë°˜í™˜
        return text


def crawl_11st_by_category(category_no: str, limit: int = 6) -> List[Dict[str, Any]]:
    """
    ìµœì‹  11ë²ˆê°€ ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ í¬ë¡¤ë§ (2025 ëŒ€ì‘)
    data-log-body ì•ˆì˜ JSONì„ íŒŒì‹±í•´ ìƒí’ˆ ì •ë³´ë¥¼ ì¶”ì¶œí•œë‹¤.
    """
    url = f"https://search.11st.co.kr/Search.tmall?ctgrNo={category_no}"
    html = _fetch_html(url)
    soup = BeautifulSoup(html, "html.parser")

    results = []

    for tag in soup.find_all(attrs={"data-log-body": True}):
        raw = tag.get("data-log-body")
        data = _parse_json_attr(raw)
        if not isinstance(data, dict):
            continue

        product_id = data.get("content_no") or data.get("productNo")
        if not product_id:
            continue

        # redirect URL ì¶”ì¶œ
        product_url = None
        link_url = data.get("link_url")
        if link_url and "redirect=" in link_url:
            try:
                parsed = urlparse(link_url)
                qs = parse_qs(parsed.query)
                product_url = qs.get("redirect", [None])[0]
            except:
                pass

        if not product_url:
            product_url = f"https://www.11st.co.kr/products/{product_id}"

        name = (
                data.get("productName")
                or data.get("snippet_object", {}).get("name")
                or ""
        )
        if not name:
            continue

        price = (
                data.get("last_discount_price")
                or data.get("productPrice")
                or None
        )
        price = _clean_price(str(price)) if price else None

        img_url = data.get("productImageUrl") or data.get("imageUrl")

        delivery = data.get("snippet_object", {}).get("delivery_price")

        results.append({
            "productName": name,
            "price": price,
            "productUrl": product_url,
            "imageUrl": img_url,
            "source": "11ë²ˆê°€",
            "deliveryPrice": delivery,
            "reason": "11ë²ˆê°€ ì¹´í…Œê³ ë¦¬ data-log-body",
        })

        if len(results) >= limit:
            break

    return results


# =========================
# ë©”ì¸ ë¶„ì„ ë¡œì§
# =========================

def run_diarization(audio_path: str) -> List[Dict[str, Any]]:
    """
    pyannoteë¡œ í™”ì ë¶„ë¦¬ ìˆ˜í–‰.
    return í˜•ì‹: [{"speaker": "SPEAKER_00", "start": 0.3, "end": 4.2}, ...]
    """
    diarization = diarization_pipeline(audio_path, AudioDecoder=AudioDecoder)

    segments = []
    for turn, _, speaker in diarization.itertracks(yield_label=True):
        segments.append({
            "speaker": speaker,
            "start": float(turn.start),
            "end": float(turn.end),
        })

    return segments


def transcribe_segment(audio_segment_path: str, language: str = "ko") -> str:
    """
    Whisperë¡œ í•œ ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•´ STT ìˆ˜í–‰.
    """
    text = ""
    segments, _ = whisper_model.transcribe(
        audio_segment_path,
        language=language,
        beam_size=5,
        vad_filter=True
    )
    for seg in segments:
        text += seg.text + " "
    return text.strip()


def process_audio_file(audio_path: str) -> Dict[str, Any]:
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸:
    - diarization
    - ê° í™”ìë³„ë¡œ ì˜¤ë””ì˜¤ ìë¥´ê¸° + STT
    - í™”ìë³„ ìš”ì•½
    - ì „ì²´ ìš”ì•½
    """
    diarization_segments = run_diarization(audio_path)

    # í™”ìë³„ í…ìŠ¤íŠ¸ ëª¨ìœ¼ê¸°
    speaker_texts: Dict[str, str] = {}
    speaker_segments: Dict[str, List[Dict[str, float]]] = {}

    for seg in diarization_segments:
        speaker = seg["speaker"]
        start = seg["start"]
        end = seg["end"]

        # ì„¸ê·¸ë¨¼íŠ¸ ì˜¤ë””ì˜¤ ì„ì‹œ íŒŒì¼ë¡œ ìë¥´ê¸°
        seg_tmp_path = os.path.join(
            tempfile.gettempdir(),
            f"{uuid.uuid4().hex}.wav"
        )
        cut_audio_segment(audio_path, start, end, seg_tmp_path)

        # Whisperë¡œ í…ìŠ¤íŠ¸ ë³€í™˜
        try:
            text = transcribe_segment(seg_tmp_path, language="ko")
        finally:
            # ì„ì‹œ ì„¸ê·¸ë¨¼íŠ¸ ì˜¤ë””ì˜¤ ì‚­ì œ
            if os.path.exists(seg_tmp_path):
                os.remove(seg_tmp_path)

        if not text:
            continue

        speaker_texts.setdefault(speaker, "")
        speaker_texts[speaker] += " " + text

        speaker_segments.setdefault(speaker, [])
        speaker_segments[speaker].append({"start": start, "end": end})

    # í™”ìë³„ ìš”ì•½
    speaker_summaries = {
        speaker: summarize_text(text, ratio=0.2, max_sentences=5)
        for speaker, text in speaker_texts.items()
    }

    # ì „ì²´ ìš”ì•½
    full_text = " ".join(speaker_texts.values())
    meeting_summary = summarize_text(full_text, ratio=0.15, max_sentences=7)

    # ì‘ë‹µ êµ¬ì¡°ë¡œ ì •ë¦¬
    speakers_result = []
    for speaker_id, full_text in speaker_texts.items():
        speakers_result.append({
            "id": speaker_id,
            "summary": speaker_summaries.get(speaker_id, ""),
            "full_text": full_text.strip(),
            "segments": speaker_segments.get(speaker_id, [])
        })

    result = {
        "speakers": speakers_result,
        "meeting_summary": meeting_summary,
    }
    return result


# =========================
# ê°„ë‹¨í•œ ì‡¼í•‘ í¬ë¡¤ëŸ¬ (DeviceMart, 11ë²ˆê°€)
# =========================

DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8",
}


def _clean_price(text: str) -> Optional[int]:
    digits = re.sub(r"[^0-9]", "", text)
    if not digits:
        return None
    try:
        return int(digits)
    except ValueError:
        return None


def _fetch_html(url: str) -> str:
    resp = requests.get(url, headers=DEFAULT_HEADERS, timeout=10)
    resp.raise_for_status()
    return resp.text


def _decode_json_string(value: str) -> str:
    """HTML ë‚´ JSON ë¸”ë¡­ì—ì„œ ì¶”ì¶œí•œ ë¬¸ìì—´ì˜ \\uXXXX ë“±ì„ ë””ì½”ë“œ."""
    try:
        return _json_unescape.loads(f'"{value}"')
    except Exception:
        return value


def _parse_json_attr(value: str) -> Optional[Dict[str, Any]]:
    try:
        return json.loads(value)
    except Exception:
        try:
            return _json_unescape.loads(value)
        except Exception:
            return None


def _extract_11st_product_info_from_url(url: str) -> Dict[str, Optional[str]]:
    info = {"product_id": None, "category": None}
    parsed = urlparse(url)

    # 1) productId ì¶”ì¶œ
    m = re.search(r"/products/(\d+)", parsed.path)
    if m:
        info["product_id"] = m.group(1)

    # 2) URL ì¿¼ë¦¬ì—ì„œ ì¹´í…Œê³ ë¦¬ ë¨¼ì € í™•ì¸ (ìµœìš°ì„ )
    qs = parse_qs(parsed.query)
    if "trCtgrNo" in qs and qs["trCtgrNo"]:
        info["category"] = qs["trCtgrNo"][0]
        return info

    # 3) HTML fallback
    try:
        html = _fetch_html(url)
    except Exception:
        return info

    cat_patterns = [
        r'dispCtgrNo"\s*[:=]\s*"?(?P<num>\d+)',
        r'ctgrNo"\s*[:=]\s*"?(?P<num>\d+)',
        r'categoryNo"\s*[:=]\s*"?(?P<num>\d+)',
    ]

    for pat in cat_patterns:
        mm = re.search(pat, html)
        if mm:
            info["category"] = mm.group("num")
            break

    return info



def _fetch_11st_category_info(disp_ctgr_no: str) -> List[Dict[str, str]]:
    """
    11ë²ˆê°€ ì¹´í…Œê³ ë¦¬ ì„œë¹„ìŠ¤ë¡œ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ ì¡°íšŒ.
    """
    url = f"http://api.11st.co.kr/rest/cateservice/category/{disp_ctgr_no}"
    try:
        resp = requests.get(url, headers=DEFAULT_HEADERS, timeout=10)
        resp.raise_for_status()
        root = ET.fromstring(resp.text)
    except Exception:
        return []

    categories: List[Dict[str, str]] = []
    for cat in root.findall(".//{*}category"):
        categories.append(
            {
                "depth": (cat.findtext("depth") or "").strip(),
                "dispNm": (cat.findtext("dispNm") or "").strip(),
                "dispNo": (cat.findtext("dispNo") or "").strip(),
                "parentDispNo": (cat.findtext("parentDispNo") or "").strip(),
            }
        )
    return categories


def _is_relevant(name: str, query: str, *, min_matches: int = 2) -> bool:
    """
    ê°„ë‹¨í•œ í† í° ê¸°ë°˜ ë§¤ì¹­: ê²€ìƒ‰ì–´ í† í° ì¼ë¶€ê°€ ìƒí’ˆëª…ì— í¬í•¨ë˜ë©´ ê´€ë ¨ì„± ìˆë‹¤ê³  íŒë‹¨.
    - min_matches: ì´ ìˆ˜ë§Œí¼ í† í°ì´ í¬í•¨ë˜ì–´ì•¼ í•¨ (ê¸°ë³¸ 2ê°œ). í† í°ì´ 1ê°œë¿ì´ë©´ 1ê°œ ë§¤ì¹­ë§Œ ìš”êµ¬.
    """
    name_lower = name.lower()
    tokens = [t for t in re.split(r"[\s\-]+", query.lower()) if len(t) >= 2]
    if not tokens:
        return True
    match_count = sum(1 for tok in tokens if tok in name_lower)
    required = min(min_matches, len(tokens))
    return match_count >= required


def _dedupe_by_name(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    seen = set()
    deduped: List[Dict[str, Any]] = []
    for item in items:
        name = item.get("productName", "").strip()
        if not name:
            continue
        key = name.lower()
        if key in seen:
            continue
        seen.add(key)
        deduped.append(item)
    return deduped


def crawl_devicemart(query: str, limit: int = 3) -> List[Dict[str, Any]]:
    search_url = f"https://www.devicemart.co.kr/goods/search?searchword={quote_plus(query)}"
    html = _fetch_html(search_url)
    soup = BeautifulSoup(html, "html.parser")

    def _collect(min_matches: int) -> List[Dict[str, Any]]:
        collected: List[Dict[str, Any]] = []
        for card in soup.select("li"):
            link_tag = card.find("a", href=re.compile(r"/goods/view"))
            if not link_tag or not link_tag.get("href"):
                continue

            name = link_tag.get_text(" ", strip=True)
            if not name:
                continue

            if not _is_relevant(name, query, min_matches=min_matches):
                continue

            price_tag = card.find("strong", class_=re.compile("price")) or card.find("span", class_=re.compile("price"))
            price = _clean_price(price_tag.get_text(" ", strip=True)) if price_tag else None
            img_tag = card.find("img")
            img_url = urljoin("https://www.devicemart.co.kr", img_tag["src"]) if img_tag and img_tag.get("src") else None
            # ì¹´í…Œê³ ë¦¬/í”Œë ˆì´ìŠ¤í™€ë” ì´ë¯¸ì§€ëŠ” ê±´ë„ˆë›´ë‹¤.
            if img_url and "/category/" in img_url:
                img_url = None

            collected.append(
                {
                    "productName": name,
                    "price": price,
                    "productUrl": urljoin("https://www.devicemart.co.kr", link_tag["href"]),
                    "imageUrl": img_url,
                    "source": "DeviceMart",
                    "deliveryPrice": None,
                    "estimatedDelivery": None,
                    "reason": "í¬ë¡¤ë§ ê²°ê³¼(ë””ë°”ì´ìŠ¤ë§ˆíŠ¸) ìƒìœ„ ë…¸ì¶œ",
                }
            )
            if len(collected) >= limit:
                break
        return collected

    results = _collect(min_matches=2)
    if len(results) < limit:
        results.extend(_collect(min_matches=1))
        results = _dedupe_by_name(results)
    return results[:limit]


def crawl_11st(query: str, limit: int = 3, category: Optional[str] = None) -> List[Dict[str, Any]]:
    query = unquote(query).strip()
    if query.startswith("http"):
        info = _extract_11st_product_info_from_url(query)
        category_no = info.get("category")
        if category_no:
            return crawl_11st_by_category(category_no, limit=limit)

    # ğŸ”¥ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•˜ëŠ” ë¶€ë¶„ â€” ë„ˆ ì½”ë“œì—ëŠ” ì—†ìŒ
    search_url = f"https://search.11st.co.kr/Search.tmall?kwd={quote_plus(query)}"
    html = _fetch_html(search_url)
    soup = BeautifulSoup(html, "html.parser")


    def _collect_from_cards(min_matches: Optional[int]) -> List[Dict[str, Any]]:
        collected: List[Dict[str, Any]] = []
        for card in soup.select("div.c_listing li, ul.c_listing li, div.listing ul li"):
            link_tag = card.find("a", href=re.compile(r"11st\\.co\\.kr/products/"))
            if not link_tag or not link_tag.get("href"):
                continue

            name = link_tag.get_text(" ", strip=True)
            if not name:
                continue

            if min_matches is not None and not _is_relevant(name, query, min_matches=min_matches):
                continue

            price_tag = card.find("strong", class_=re.compile("price")) or card.find("span", class_=re.compile("price"))
            price = _clean_price(price_tag.get_text(" ", strip=True)) if price_tag else None
            if price is None:
                price = _find_price_near(card)

            img_tag = card.find("img")
            img_url = None
            if img_tag:
                img_url = img_tag.get("data-original") or img_tag.get("data-src") or img_tag.get("src")
            if img_url and img_url.startswith("//"):
                img_url = "https:" + img_url

            delivery_text = None
            delivery_tag = card.find(string=re.compile("ë¬´ë£Œ"))
            if delivery_tag:
                delivery_text = "ë¬´ë£Œë°°ì†¡"

            collected.append(
                {
                    "productName": name,
                    "price": price,
                    "productUrl": link_tag["href"],
                    "imageUrl": img_url,
                    "source": "11ë²ˆê°€",
                    "deliveryPrice": delivery_text,
                    "estimatedDelivery": None,
                    "reason": "í¬ë¡¤ë§ ê²°ê³¼(11ë²ˆê°€) ìƒìœ„ ë…¸ì¶œ",
                }
            )
            if len(collected) >= limit:
                break
        return collected

    def _collect_from_links(min_matches: Optional[int]) -> List[Dict[str, Any]]:
        """
        ì¹´ë“œ ì…€ë ‰í„°ê°€ ê¹¨ì¡Œì„ ë•Œ ëŒ€ë¹„: í˜ì´ì§€ ë‚´ ëª¨ë“  product ë§í¬ë¥¼ í›‘ìœ¼ë©° ìˆ˜ì§‘.
        """
        collected: List[Dict[str, Any]] = []
        seen_links = set()
        for link_tag in soup.find_all("a", href=re.compile(r"11st\\.co\\.kr/products/")):
            href = link_tag.get("href")
            if not href or href in seen_links:
                continue
            seen_links.add(href)

            name = link_tag.get_text(" ", strip=True)
            if not name:
                continue

            if min_matches is not None and not _is_relevant(name, query, min_matches=min_matches):
                continue

            parent = link_tag.find_parent(["li", "div"]) or link_tag
            price = _find_price_near(parent)

            img_url = None
            img_tag = parent.find("img")
            if img_tag:
                img_url = img_tag.get("data-original") or img_tag.get("data-src") or img_tag.get("src")
            if img_url and img_url.startswith("//"):
                img_url = "https:" + img_url

            collected.append(
                {
                    "productName": name,
                    "price": price,
                    "productUrl": href,
                    "imageUrl": img_url,
                    "source": "11ë²ˆê°€",
                    "deliveryPrice": None,
                    "estimatedDelivery": None,
                    "reason": "í¬ë¡¤ë§ ê²°ê³¼(11ë²ˆê°€) ë§í¬ íŒŒì‹±",
                }
            )
            if len(collected) >= limit:
                break
        return collected

    def _collect_from_json_blob() -> List[Dict[str, Any]]:
        """
        HTML ë‚´ ìŠ¤í¬ë¦½íŠ¸ JSONì—ì„œ product ì •ë³´ë¥¼ ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œ.
        """
        collected: List[Dict[str, Any]] = []
        pattern = re.compile(
            r'"productname"\\s*:\\s*"(?P<name>[^"]+?)".*?"productid"\\s*:\\s*"?(?P<pid>\\d+)"?.*?"productprice"\\s*:\\s*"?(?P<price>\\d+)"?.*?"productimage"\\s*:\\s*"(?P<img>[^"]+?)"',
            re.DOTALL | re.IGNORECASE,
        )
        for m in pattern.finditer(html):
            name_raw = m.group("name")
            pid = m.group("pid")
            price_val = _clean_price(m.group("price"))
            img_url = m.group("img")
            name = _decode_json_string(name_raw)
            if not name:
                continue
            collected.append(
                {
                    "productName": name,
                    "price": price_val,
                    "productUrl": f"https://www.11st.co.kr/products/{pid}",
                    "imageUrl": img_url,
                    "source": "11ë²ˆê°€",
                    "deliveryPrice": None,
                    "estimatedDelivery": None,
                    "reason": "í¬ë¡¤ë§ ê²°ê³¼(11ë²ˆê°€) JSON íŒŒì‹±",
                }
            )
            if len(collected) >= limit:
                break
        return collected

    def _collect_from_data_log_body() -> List[Dict[str, Any]]:
        collected = []

        for tag in soup.find_all(attrs={"data-log-body": True}):
            raw = tag.get("data-log-body")
            data = _parse_json_attr(raw)
            if not isinstance(data, dict):
                continue

            product_id = data.get("content_no") or data.get("productNo")
            if not product_id:
                continue

            # ê´‘ê³  redirect URLì—ì„œ ì§„ì§œ ìƒí’ˆURL ì¶”ì¶œ
            link_url = data.get("link_url")
            product_url = None
            if link_url and "redirect=" in link_url:
                try:
                    parsed = urlparse(link_url)
                    qs = parse_qs(parsed.query)
                    product_url = qs.get("redirect", [None])[0]
                except:
                    pass

            if not product_url:
                product_url = f"https://www.11st.co.kr/products/{product_id}"

            # ìƒí’ˆëª…
            name = data.get("productName") or data.get("snippet_object", {}).get("name") or ""
            if not name:
                continue

            # ê°€ê²©
            price = data.get("last_discount_price") or data.get("productPrice")
            price = _clean_price(str(price)) if price else None

            # ì´ë¯¸ì§€
            img_url = data.get("productImageUrl") or data.get("imageUrl")

            # ë°°ì†¡ë¹„
            delivery = data.get("snippet_object", {}).get("delivery_price")

            collected.append({
                "productName": name,
                "price": price,
                "productUrl": product_url,
                "imageUrl": img_url,
                "source": "11ë²ˆê°€",
                "deliveryPrice": delivery,
                "reason": "11ë²ˆê°€ data-log-body JSON",
            })

            if len(collected) >= limit:
                break

        return collected


def crawl_products(query: str, limit_total: int = 6, sources: Optional[List[str]] = None, category: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    ê²€ìƒ‰ì–´ë¡œ ê° ì‡¼í•‘ëª°ì„ í¬ë¡¤ë§.
    - sources: ["devicemart", "11st"] ì¤‘ ì„ íƒ. Noneì´ë©´ ë‘˜ ë‹¤.
    """
    normalized = [s.lower() for s in sources] if sources else ["devicemart", "11st"]
    items: List[Dict[str, Any]] = []

    crawlers: List = []
    if any(s in normalized for s in ("devicemart", "device", "dm")):
        crawlers.append(crawl_devicemart)
    if any(s in normalized for s in ("11st", "11ë²ˆê°€", "eleven")):
        crawlers.append(crawl_11st)
    if not crawlers:
        crawlers = [crawl_devicemart, crawl_11st]

    per_site = max(1, limit_total // max(1, len(crawlers)))

    for crawler in crawlers:
        try:
            if crawler is crawl_11st:
                items.extend(crawler(query, limit=per_site, category=category))
            else:
                items.extend(crawler(query, limit=per_site))
        except Exception as e:
            # í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ì‚¬ì´íŠ¸ë¼ë„ ê³„ì† ì‹œë„
            print(f"[crawler] {crawler.__name__} failed: {e}")

    items = _dedupe_by_name(items)
    return items[:limit_total]


# =========================
# FastAPI ì—”ë“œí¬ì¸íŠ¸
# =========================


@app.post("/recommendations/prompt")
async def recommendation_prompt(payload: RecommendationRequest):
    """
    ê¸°ì¤€ ìƒí’ˆ + í›„ë³´ ëª©ë¡ì„ ë°›ì•„ Geminiì— ì „ë‹¬í•  system/user í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œë¥¼ ë°˜í™˜,
    ë™ì‹œì— Geminië¥¼ í˜¸ì¶œí•´ ê²°ê³¼ë¥¼ í•¨ê»˜ ì œê³µ.
    """
    prompts = build_recommendation_prompt(payload.base_item, payload.candidates)
    try:
        gemini_result = call_gemini(prompts["system_prompt"], prompts["user_prompt"])
        return JSONResponse(
            content={
                **prompts,
                "gemini_raw": gemini_result["raw"],
                "gemini_parsed": gemini_result["parsed"],
            }
        )
    except HTTPException as e:
        # 429 ë“± LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œì—ë„ í”„ë¡¬í”„íŠ¸ëŠ” ë‚´ë ¤ì„œ í”„ëŸ°íŠ¸ê°€ ì§ì ‘ í˜¸ì¶œí•˜ê±°ë‚˜ ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡ í•œë‹¤.
        if e.status_code == 429:
            return JSONResponse(
                status_code=200,
                content={
                    **prompts,
                    "gemini_raw": None,
                    "gemini_parsed": None,
                    "gemini_error": "Gemini í˜¸ì¶œì´ ì œí•œë˜ì—ˆìŠµë‹ˆë‹¤. í”„ëŸ°íŠ¸ì—ì„œ ì§ì ‘ í˜¸ì¶œí•˜ê±°ë‚˜ ì ì‹œ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.",
                },
            )
        raise


@app.post("/analyze")
async def analyze_meeting_audio(file: UploadFile = File(...)):
    """
    íšŒì˜ ìŒì„± íŒŒì¼ ì—…ë¡œë“œ â†’ í™”ìë³„ ìš”ì•½ + ì „ì²´ ìš”ì•½ ë°˜í™˜
    """
    if not file.filename:
        raise HTTPException(status_code=400, detail="íŒŒì¼ ì´ë¦„ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ê°„ë‹¨í•œ í™•ì¥ì ì²´í¬ (ì›í•˜ë©´ ë” ê°•í™” ê°€ëŠ¥)
    if not (file.filename.endswith(".wav") or file.filename.endswith(".mp3") or file.filename.endswith(".m4a")):
        raise HTTPException(status_code=400, detail="wav/mp3/m4a í˜•ì‹ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")

    tmp_path = save_upload_file_tmp(file)

    try:
        result = process_audio_file(tmp_path)
        return JSONResponse(content=result)
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        # ì›ë³¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(tmp_path):
            os.remove(tmp_path)

# pyannote 3.3.xì—ì„œ AudioDecoder ì‹¬ë³¼ ëˆ„ë½ ì‹œ diarization_pipeline í˜¸ì¶œì— ë„˜ê²¨ì£¼ê¸° ìœ„í•œ fallback
from fastapi import Request


@app.post("/debug/pyannote")
async def debug_pyannote(request: Request):
    """
    pyannote AudioDecoderê°€ import ê°€ëŠ¥í•œì§€ í™•ì¸ìš© ê°„ë‹¨ ì—”ë“œí¬ì¸íŠ¸
    """
    ok = AudioDecoder is not None
    return {"AudioDecoder_present": ok}


@app.get("/health")
async def health_check():
    return {"status": "ok"}


@app.get("/recommendations/crawl")
async def crawl_recommendations(q: str, limit: int = 6, source: Optional[str] = None, category: Optional[str] = None):
    """
    Gemini ì—†ì´ ê°„ë‹¨íˆ ê²€ìƒ‰ì–´ ê¸°ë°˜ í¬ë¡¤ë§ìœ¼ë¡œ ìƒìœ„ ìƒí’ˆì„ ìˆ˜ì§‘í•´ ë°˜í™˜í•œë‹¤.
    - q: ê²€ìƒ‰ì–´ (ì˜ˆ: ê¸°ì¤€ ìƒí’ˆëª…)
    - limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜
    - source: "devicemart" ë˜ëŠ” "11st" ì¤‘ ì„ íƒ (Noneì´ë©´ ë‘˜ ë‹¤)
    - category: 11ë²ˆê°€ ì¹´í…Œê³ ë¦¬ ë²ˆí˜¸. qê°€ 11ë²ˆê°€ ìƒí’ˆ URLì´ë©´ trCtgrNoë¥¼ ìë™ ì¶”ì¶œí•´ ì‚¬ìš©í•œë‹¤.
    """
    limit = max(1, min(10, limit))
    sources = [source] if source else None
    items = crawl_products(q, limit_total=limit, sources=sources, category=category)

    return JSONResponse(
        content={
            "query": q,
            "source": source or "all",
            "category": category,
            "count": len(items),
            "items": items,
            "note": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
            "debug": {
                "devicemart_enabled": not source or source.lower() in ("devicemart", "device", "dm"),
                "eleven_enabled": not source or source.lower() in ("11st", "11ë²ˆê°€", "eleven"),
            },
        },
        status_code=200,
    )
import requests
import xml.etree.ElementTree as ET
from fastapi import FastAPI, Query
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("jhgan/ko-sroberta-multitask")
kw_model = KeyBERT(model)

def extract_main_keyword(text: str) -> str:
    keywords = kw_model.extract_keywords(
        text,
        keyphrase_ngram_range=(1, 2),
        stop_words=None,
        top_n=10
    )
    return keywords[0][0]

API_KEY = "ff49fbaa914833d531a36ada7b3c3ac0"



def search_11st_products(keyword: str, limit: int = 20):
    url = "http://openapi.11st.co.kr/openapi/OpenApiService.tmall"
    params = {
        "key": API_KEY,
        "apiCode": "ProductSearch",
        "keyword": keyword,
        "pageSize": limit,
        "sortCd": "CP"
    }

    xml_response = requests.get(url, params=params).text
    return parse_product_xml(xml_response)


def parse_product_xml(xml_data: str):
    root = ET.fromstring(xml_data)
    products = root.find("Products")

    if products is None:
        return []

    result = []
    for product in products.findall("Product"):
        def get(tag):
            e = product.find(tag)
            return e.text if e is not None else None

        result.append({
            "productCode": get("ProductCode"),
            "name": get("ProductName"),
            "price": get("ProductPrice"),
            "image": get("ProductImage300") or get("ProductImage"),
            "detailUrl": get("DetailPageUrl"),
            "seller": get("SellerNick"),
        })

    return result


@app.get("/recommend/11st")
def recommend_from_name(
    name: str = Query(..., description="ìƒí’ˆëª… ê·¸ëŒ€ë¡œ ì…ë ¥"),
    limit: int = 20
):
    keyword = extract_main_keyword(name)
    print(" â¬‡ï¸ ì¶”ì¶œëœ í•µì‹¬ í‚¤ì›Œë“œ:", keyword)

    items = search_11st_products(keyword, limit)

    return {
        "query": name,
        "keyword": keyword,
        "count": len(items),
        "items": items
    }
